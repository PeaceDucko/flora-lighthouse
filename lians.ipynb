{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b6112f-af3d-4722-9570-a4dd1e9389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import csv\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import errno\n",
    "from operator import add\n",
    "from spacy.lang.nl.examples import sentences \n",
    "from datetime import datetime\n",
    "\n",
    "label_loc = \"api/data/label_names.csv\"\n",
    "# basepath for all files\n",
    "BasePath_f = \"api/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05bbd932-6250-4a3a-a1ba-50068378ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta:\n",
      "[{'name': 'Niet Gedetecteerd', 'data': [0.016866666666666665], 'color': '#ebebeb'}, {'name': 'Orientatie', 'data': [0.9541833333333335], 'color': '#175a91'}, {'name': 'Orientatie', 'data': [1.1941], 'color': '#175a91'}, {'name': 'Niet Gedetecteerd', 'data': [28.061666666666667], 'color': '#ebebeb'}, {'name': 'Monitoren', 'data': [0.05009999999999848], 'color': '#104469'}, {'name': 'Niet Gedetecteerd', 'data': [0.05341666666666689], 'color': '#ebebeb'}, {'name': 'Monitoren', 'data': [0.050016666666669596], 'color': '#104469'}, {'name': 'Niet Gedetecteerd', 'data': [11.26018333333333], 'color': '#ebebeb'}, {'name': 'Monitoren', 'data': [0.050233333333332686], 'color': '#104469'}, {'name': 'Niet Gedetecteerd', 'data': [0.02393333333333525], 'color': '#ebebeb'}, {'name': 'Monitoren', 'data': [0.031466666666667975], 'color': '#104469'}, {'name': 'Monitoren', 'data': [0.20239999999999703], 'color': '#104469'}, {'name': 'Niet Gedetecteerd', 'data': [3.0514000000000014], 'color': '#ebebeb'}]\n",
      ", m_perc:\n",
      "[{'name': 'Orientatie', 'data': 0.04773962962962963}, {'name': 'Plannen', 'data': 0.0}, {'name': 'Evaluatie', 'data': 0.0}, {'name': 'Monitoren', 'data': 0.008538148148148128}]\n",
      ", cog:\n",
      "[{'name': 'Niet Gedetecteerd', 'data': [1.6666666666666667e-05], 'color': '#ebebeb'}, {'name': 'ESSAY_TASK_START', 'data': [0.016833333333333336], 'color': '#000000'}, {'name': 'Niet Gedetecteerd', 'data': [2.3042000000000002], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [0.49910000000000015], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.054566666666666305], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.0014999999999998348], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.02013333333333346], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [0.2942666666666666], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.07220000000000025], 'color': '#dd1d08'}, {'name': 'Herlezen', 'data': [0.002716666666666978], 'color': '#e34d4e'}, {'name': 'Niet Gedetecteerd', 'data': [0.02088333333333338], 'color': '#ebebeb'}, {'name': 'Herlezen', 'data': [1.3831499999999999], 'color': '#e34d4e'}, {'name': 'Niet Gedetecteerd', 'data': [0.025400000000000283], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.6309333333333331], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.03138333333333347], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [2.066916666666667], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.04948333333333402], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.0012833333333329977], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.05159999999999956], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [1.8876666666666668], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.09498333333333359], 'color': '#dd1d08'}, {'name': 'Herlezen', 'data': [0.6366333333333336], 'color': '#e34d4e'}, {'name': 'Niet Gedetecteerd', 'data': [0.01994999999999955], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [3.2256166666666677], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.03313333333333307], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.3207333333333337], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.01909999999999773], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [7.566533333333336], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.23673333333333296], 'color': '#dd1d08'}, {'name': 'Herlezen', 'data': [0.139733333333332], 'color': '#e34d4e'}, {'name': 'Niet Gedetecteerd', 'data': [0.022066666666666346], 'color': '#ebebeb'}, {'name': 'Herlezen', 'data': [0.5729500000000015], 'color': '#e34d4e'}, {'name': 'Niet Gedetecteerd', 'data': [0.020766666666665323], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [1.1495166666666659], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.02171666666666905], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.2503666666666682], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.09644999999999737], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [6.141566666666669], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.201733333333331], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.042100000000001025], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.05013333333333203], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.05338333333333334], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.05005000000000315], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.19876666666666487], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.1981166666666656], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [0.4327666666666691], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.1213000000000003], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.08131666666666482], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.15146666666666475], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [3.5157500000000006], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.06763333333333177], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.08790000000000131], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.176516666666669], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.22414999999999963], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.34869999999999624], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [5.2741833333333386], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.3799666666666679], 'color': '#dd1d08'}, {'name': 'Lezen', 'data': [0.0015166666666666107], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.05026666666666624], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.023900000000001698], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.23391666666666533], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.4722500000000013], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.01646666666666463], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.23031666666666673], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.02101666666666946], 'color': '#ebebeb'}, {'name': 'Lezen', 'data': [0.1137500000000008], 'color': '#690008'}, {'name': 'Niet Gedetecteerd', 'data': [0.29825000000000046], 'color': '#ebebeb'}, {'name': 'Schrijven', 'data': [1.8700666666666628], 'color': '#dd1d08'}, {'name': 'Schrijven', 'data': [0.03646666666666576], 'color': '#dd1d08'}]\n",
      ", c_perc:\n",
      "[{'name': 'Lezen', 'data': 0.15795911248307226}, {'name': 'Herlezen', 'data': 0.060772105806735456}, {'name': 'Schrijven', 'data': 0.6857540994351649}]\n",
      ", pplg:\n",
      "[7.33, 7.33, 0.0]\n",
      ", personal:\n",
      "{'OrientatieMins': 2.1482833333333335, 'OrientatieStart': 0.016866666666666665, 'PlannenMins': 0.0, 'PlannenStart': 0, 'EvaluatieMins': 0.0, 'EvaluatieStart': 0, 'MonitorenMins': 0.38421666666666576, 'MonitorenStart': 30.226833333333335, 'LezenMins': 7.109300000000004, 'LezenStart': 2.87475, 'HerlezenMins': 2.735183333333334, 'HerlezenStart': 3.262883333333333, 'SchrijvenMins': 30.86388333333334, 'SchrijvenStart': 2.32105}\n",
      ", spiderData:\n",
      "[0.41528696961702594, 0.5111797848324953, 1.0875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3080686/965610956.py:209: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  source_overlap_1.append(doc_1.similarity(l[sent]))\n",
      "/tmp/ipykernel_3080686/965610956.py:210: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  source_overlap_2.append(doc_2.similarity(l[sent]))\n",
      "/tmp/ipykernel_3080686/965610956.py:211: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  source_overlap_3.append(doc_3.similarity(l[sent]))\n",
      "/tmp/ipykernel_3080686/965610956.py:219: UserWarning: [W008] Evaluating Span.similarity based on empty vectors.\n",
      "  cohesion.append(l[j].similarity(l[k]))\n"
     ]
    }
   ],
   "source": [
    "# reading in the label csv to assign each pattern label to a type and sub type\n",
    "def load_label_meanings():\n",
    "  # =============================================================================== label names is constant and used to map individual labels to the parent process\n",
    "  labels_df = pd.read_csv(label_loc)\n",
    "  sub_dict = {}\n",
    "  main_dict = {}\n",
    "  color_dict = {}\n",
    "\n",
    "  # reading in the type of each pattern label and creating a dictionary for mapping\n",
    "  for index, row in labels_df.iterrows():\n",
    "    sub_dict[row[\"Pattern No.\"]] = row[\"Sub-category\"]\n",
    "    color_dict[row[\"Pattern No.\"]] = row[\"color\"]\n",
    "    m_pattern = row[\"Pattern No.\"]\n",
    "    if m_pattern[0] == \"M\":\n",
    "      main_dict[row[\"Pattern No.\"]] = \"Metacognition\"\n",
    "    else:\n",
    "      main_dict[row[\"Pattern No.\"]] = \"Cognition\"\n",
    "    main_dict[\"NO_PATTERN\"] = \"NO_PATTERN\"\n",
    "\n",
    "  return sub_dict, main_dict, color_dict\n",
    "\n",
    "def load_process_features_study_f(BasePath_f, sub_dict, main_dict, color_dict, f):\n",
    "\n",
    "  # getting the data of the specific student\n",
    "  # ==================================================================================================== here we read the pattern labels from the flora server\n",
    "  data = pd.read_csv(BasePath_f + f)\n",
    "  data = data[data[\"Process End Time\"] > -1]\n",
    "\n",
    "  # scaling between 0 and 1\n",
    "  # max_time = np.max(data[\"Process End Time\"])\n",
    "\n",
    "  # this makes 45 minutes the maximum time the bar chart is filled out to\n",
    "  max_time = 2700000\n",
    "\n",
    "  data[\"Process End Time\"] = data[\"Process End Time\"] / max_time  \n",
    "  data[\"Process Start Time\"] = data[\"Process Start Time\"] / max_time\n",
    "\n",
    "  # adding extra columns to the data frame\n",
    "  data[\"Process_Time_Spent\"] = data[\"Process End Time\"] - data[\"Process Start Time\"]\n",
    "  data[\"Process_sub\"] = data[\"Process Label\"].map(sub_dict) \n",
    "  data[\"Process_main\"] = data[\"Process Label\"].map(main_dict) \n",
    "  data[\"Color\"] = data[\"Process Label\"].map(color_dict)\n",
    "\n",
    "  time_scaler = max_time / 60000\n",
    "\n",
    "  # return the full user df\n",
    "  return data, time_scaler\n",
    "\n",
    "# with the full data of the user we want two series\n",
    "# one series of the meta and one of the cog\n",
    "# a series is a list of dictionaries that contain the sub process label and the time spent on that sub process\n",
    "# in between each sub process there should be a dedicated \"BLANK\" section for if there is no label detected\n",
    "def create_series(df, cog_type, time_scaler):\n",
    "  # blank colour picker:\n",
    "  blank_colour = \"#ebebeb\"\n",
    "\n",
    "  # selecting the correct type of labels\n",
    "  m_df = df[df[\"Process_main\"] == cog_type]\n",
    "  m_df = m_df[[\"Process Start Time\",\t\"Process End Time\",\t\"Process_Time_Spent\",\t\"Process_sub\", \"Color\"]].reset_index(inplace = False)\n",
    "\n",
    "  # adds a blank at the start since not both meta and cog can have the first label\n",
    "  line = pd.DataFrame({\"Process Start Time\": 0, \"Process End Time\": m_df.iloc[0, 1], \"Process_Time_Spent\": m_df.iloc[0, 1], \"Process_sub\": \"Niet Gedetecteerd\", \"Color\": blank_colour}, index=[0])\n",
    "\n",
    "  # concatenate two dataframe\n",
    "  m_df = pd.concat([line, m_df]).reset_index(drop = True)\n",
    "  m_df = m_df[[\"Process Start Time\",\t\"Process End Time\",\t\"Process_Time_Spent\",\t\"Process_sub\", \"Color\"]].reset_index(drop = True)\n",
    "\n",
    "  # now we iterate through each row of the df and if there is a gap between two processes we fill the gap with a BLANK\n",
    "  m_np = []\n",
    "  for i, row in m_df.iterrows():\n",
    "    m_row = []\n",
    "    if (row[\"Process Start Time\"] - m_df.iloc[i-1, 1]) > 0.00001:\n",
    "      m_np.append([m_df.iloc[i-1, 1], row[\"Process Start Time\"], row[\"Process Start Time\"] - m_df.iloc[i-1, 1], \"Niet Gedetecteerd\", blank_colour])\n",
    "    m_np.append(row.to_list())\n",
    "    \n",
    "  # adding a blank at the end in case the last process is of the other type of label\n",
    "  m_np.append([m_df.iloc[-1, 1], 1, 1 - m_df.iloc[-1, 1], \"Niet Gedetecteerd\", blank_colour])\n",
    "  m_df = pd.DataFrame(m_np, columns = [\"Process Start Time\",\t\"Process End Time\",\t\"Process_Time_Spent\",\t\"Process_sub\", \"Color\"])\n",
    "  m_df[\"Process_Time_Spent\"] = m_df[\"Process_Time_Spent\"] * time_scaler\n",
    "  m_df[\"Process End Time\"] = m_df[\"Process End Time\"] * time_scaler\n",
    "  m_df[\"Process Start Time\"] = m_df[\"Process Start Time\"] * time_scaler\n",
    "\n",
    "\n",
    "  # now we go through the data frame and attach labels that are the same to each other\n",
    "  # ============================ DOESNT WORK BUT PLEASE DONT UNCOMMENT ============================\n",
    "\n",
    "  # drop_list = []\n",
    "  # cleaned = []\n",
    "  # for i, row in m_df.iterrows():\n",
    "  #   inp = row.to_list()\n",
    "  #   if row[\"Process_sub\"] == m_df.iloc[i-1, 3]:\n",
    "  #     print(\"true\")\n",
    "  #     print(\"current:\",inp)\n",
    "  #     inp[0] = m_df.iloc[i-2, 1]\n",
    "  #     inp[2] = row[\"Process_Time_Spent\"] + m_df.iloc[i-1, 2]\n",
    "  #     drop_list.append(i)\n",
    "    \n",
    "  #   print(\"final\",inp)\n",
    "  #   print(\"-----\")\n",
    "  #   cleaned.append(inp)\n",
    "  # m_df = pd.DataFrame(m_np, columns = [\"Process Start Time\",\t\"Process End Time\",\t\"Process_Time_Spent\",\t\"Process_sub\", \"Color\"])\n",
    "  # ===============================================================================================\n",
    "\n",
    "  # having created the dataframe we now just have to create the series of data\n",
    "  series = []\n",
    "  for i, row in m_df.iterrows():\n",
    "    if (row[\"Process_Time_Spent\"] > 0):\n",
    "      row_dic = {}\n",
    "      row_dic[\"name\"] = row[\"Process_sub\"]\n",
    "      row_dic[\"data\"] = [row[\"Process_Time_Spent\"]]\n",
    "      row_dic[\"color\"] = row[\"Color\"]\n",
    "      series.append(row_dic)\n",
    "\n",
    "  # the order specified\n",
    "  orders = {}\n",
    "  orders[\"Metacognition\"] = [\"Orientatie\", \"Plannen\", \"Evaluatie\", \"Monitoren\"]\n",
    "  orders[\"Cognition\"] = [\"Lezen\", \"Herlezen\", \"Schrijven\"]\n",
    "\n",
    "  # getting the percentages of each process, along with time until started and time spent on it\n",
    "  perc = []\n",
    "  personal = {}\n",
    "  process_order = list(m_df[\"Process_sub\"])\n",
    "  # print(m_df)\n",
    "\n",
    "  for i in orders[cog_type]:\n",
    "    row_dic = {}\n",
    "    row_dic[\"name\"] = i\n",
    "\n",
    "    # percentage\n",
    "    row_dic[\"data\"] = m_df[m_df[\"Process_sub\"] == i][\"Process_Time_Spent\"].sum() / (m_df[\"Process End Time\"].max())\n",
    "\n",
    "    # minutes spent on it\n",
    "    personal[i+\"Mins\"] = m_df[m_df[\"Process_sub\"] == i][\"Process_Time_Spent\"].sum()\n",
    "\n",
    "    # started at minute:\n",
    "    if (i in process_order):\n",
    "      personal[i+\"Start\"] = m_df[m_df[\"Process_sub\"] == i][\"Process Start Time\"].min()\n",
    "    else:\n",
    "      personal[i+\"Start\"] = 0\n",
    "\n",
    "    perc.append(row_dic)\n",
    "  return [series, perc, personal]\n",
    "\n",
    "# getting and returning the pre, post and learning gain of a student:\n",
    "def results(file_path, username):\n",
    "  # ==================================================================================================== questionnaire is a server call to moodle\n",
    "  pre_test_df = pd.read_csv(file_path + \"Questionnaire B.csv\")\n",
    "  pre_test = pre_test_df[pre_test_df[\"First name\"] == username].iloc[0,7]\n",
    "  pre_test = round(float(pre_test) / 1.5, 2)\n",
    "\n",
    "  # ==================================================================================================== same here\n",
    "  post_test_df = pd.read_csv(file_path + \"Questionnaire C.csv\")\n",
    "  post_test = post_test_df[post_test_df[\"First name\"] == username].iloc[0,7]\n",
    "  post_test = round(float(post_test) / 1.5, 2)\n",
    "\n",
    "  return [pre_test, post_test, round(post_test - pre_test, 2)]\n",
    "\n",
    "# Susanne's script time \n",
    "def susanneScript(username):\n",
    "\n",
    "  # Please make sure the spacy pipeline for Dutch\n",
    "  # is installed (for details: https://spacy.io/models/nl) - this should have been already done in the previous step,\n",
    "  # but it is still good to check\n",
    "  # Note: we use the model trained on a large corpus of texts, you may want to opt for a medium or small corpus instead,\n",
    "  # in case efficiency is an issue\n",
    "  nlp = spacy.load(\"nl_core_news_lg\")\n",
    "  # nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "  # Import source texts\n",
    "  # ==================================================================================================== source texts are constant so can just be on the server\n",
    "  ai = open(BasePath_f + 'spiderScript/AI_NL.rtf', 'r')\n",
    "  dif = open(BasePath_f + 'spiderScript/Differentiatie_NL.rtf', 'r')\n",
    "  sc = open(BasePath_f + 'spiderScript/Scaffolding_NL.rtf', 'r')\n",
    "\n",
    "  # Build nlp objects for texts\n",
    "  doc_1 = nlp(ai.read())\n",
    "  doc_2 = nlp(dif.read())\n",
    "  doc_3 = nlp(sc.read())\n",
    "\n",
    "  # Fetch and process each essay\n",
    "  max_numberwords = 400\n",
    "  es_source_overlap = []\n",
    "  cohesion = []\n",
    "  mean_cohesion = []\n",
    "  word_count = []\n",
    "  word_countrel =[]\n",
    "\n",
    "  # ====================================================================================== essay is chosen here\n",
    "  essay_file = BasePath_f + \"essays/\" + username + \"_essay.txt\"\n",
    "\n",
    "  essay = open(essay_file, 'r')\n",
    "  word_count = len(open(essay_file, 'r+').read().split())\n",
    "  word_countrel = word_count/ max_numberwords\n",
    "\n",
    "  # Build nlp object for the essay\n",
    "  doc_essay = nlp(essay.read())\n",
    "\n",
    "  # Tokenize essay into sentences\n",
    "  l=[]\n",
    "  for sent in doc_essay.sents:\n",
    "      l.append(sent)\n",
    "      \n",
    "  # Compute semantic overlap with sources\n",
    "  # Loop over each sentence and compute its semantic overlap with each source text\n",
    "  source_overlap_1 = []\n",
    "  source_overlap_2 = []\n",
    "  source_overlap_3 = []\n",
    "  for sent in range(len(l)):\n",
    "      source_overlap_1.append(doc_1.similarity(l[sent]))\n",
    "      source_overlap_2.append(doc_2.similarity(l[sent]))\n",
    "      source_overlap_3.append(doc_3.similarity(l[sent]))\n",
    "      \n",
    "  # Semantic overlap with sources for the essay\n",
    "  es_source_overlap = (np.mean(source_overlap_1) + np.mean(source_overlap_2) + np.mean(source_overlap_3))/5\n",
    "\n",
    "  # Compute sentence mean cohesion (combinations without repetition)\n",
    "  for k in range(0,len(l)-1):\n",
    "      for j in range(k+1,len(l)):\n",
    "          cohesion.append(l[j].similarity(l[k]))\n",
    "\n",
    "  # Mean cohesion for the essay\n",
    "  mean_cohesion = np.mean(cohesion)\n",
    "\n",
    "  # return the list of values\n",
    "  return [es_source_overlap, mean_cohesion, word_countrel]\n",
    "\n",
    "# ==================================================================================================== main\n",
    "\n",
    "# loading the maps for colour and labels of each pattern id\n",
    "sub_dict, main_dict, color_dict = load_label_meanings()\n",
    "\n",
    "\n",
    "# =============================================================================== username is set here and cannot have an underscore (_)\n",
    "user_name = \"fsh3110\"\n",
    "\n",
    "# making the pattern dataframe\n",
    "df, time_scaler = load_process_features_study_f(BasePath_f + \"processLabel/\", sub_dict, main_dict, color_dict, user_name + \"_pattern.csv\")\n",
    "\n",
    "# making the data series and percentages for meta and cog\n",
    "m_series, m_perc, m_personal = create_series(df, \"Metacognition\", time_scaler)\n",
    "c_series, c_perc, c_personal = create_series(df, \"Cognition\", time_scaler)\n",
    "\n",
    "# loading in the pre, post and learning gain results\n",
    "tests = results(BasePath_f + \"test/\", user_name)\n",
    "\n",
    "# creating the dictionary for personal feedback\n",
    "personal = m_personal\n",
    "personal.update(c_personal)\n",
    "\n",
    "# getting the essay score:\n",
    "spiderdata = susanneScript(user_name)\n",
    "\n",
    "# ====================================================================================================\n",
    "# this is how the json should look really\n",
    "print(\"meta:\")\n",
    "print(m_series)\n",
    "print(\", m_perc:\")\n",
    "print(m_perc)\n",
    "print(\", cog:\")\n",
    "print(c_series)\n",
    "print(\", c_perc:\")\n",
    "print(c_perc)\n",
    "print(\", pplg:\")\n",
    "print(tests)\n",
    "print(\", personal:\")\n",
    "print(personal)\n",
    "print(\", spiderData:\") # has order of es_source_overlap, mean_cohesion, word_countrel\n",
    "print(spiderdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49beb9-b833-4641-b379-086b6a1b89c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
